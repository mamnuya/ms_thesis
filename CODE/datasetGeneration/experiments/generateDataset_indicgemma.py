import json
import itertools
import torch
import re
import sys
import os
from unsloth import FastLanguageModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

'''
!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121 !pip install "unsloth[kaggle-new] @git+https://github.com/unslothai/unsloth.git@nightly"


pip install "cut-cross-entropy @ git+https://github.com/apple/ml-cross-entropy.git"


pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git git+https://github.com/unslothai/unsloth-zoo.git

'''

# Add the directory containing IndicTransToolkit to sys.path
sys.path.append('/scratch/mrinki/ms_thesis/IndicTransToolkit')
from IndicTransToolkit import IndicProcessor

cache_dir = "/scratch/mrinki/huggingface_cache"

# Load Indic-Gemma 7B (Navarasa-2.0) with Fast Inference
#max_seq_length = 2048
#dtype = None  # Auto-detect: Use Float16 for Tesla T4/V100, BFloat16 for Ampere+
#load_in_4bit = False  

indic_model_name = "Telugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa-2.0"
indic_model, indic_tokenizer = FastLanguageModel.from_pretrained(
    model_name=indic_model_name,
    device_map="auto",
    cache_dir=cache_dir
)

FastLanguageModel.for_inference(indic_model)  # Enable 2x faster inference

# Load AI4Bharat Translation Model
ai4bharat_model_name = "ai4bharat/indictrans2-indic-en-1B"
ai4bharat_tokenizer = AutoTokenizer.from_pretrained(ai4bharat_model_name, trust_remote_code=True, cache_dir=cache_dir)
ai4bharat_model = AutoModelForSeq2SeqLM.from_pretrained(
    ai4bharat_model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    cache_dir=cache_dir,
    trust_remote_code=True
)

processor = IndicProcessor(inference=True)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Language Mapping
def language_to_src_code(language):
    mapping = {
        "Hindi": "hin_Deva", "Urdu": "urd_Arab", "Bengali": "ben_Beng", "Punjabi": "pan_Guru",
        "Marathi": "mar_Deva", "Gujarati": "guj_Gujr", "Malayalam": "mal_Mlym",
        "Tamil": "tam_Taml", "Telugu": "tel_Telu", "Kannada": "kan_Knda"
    }
    return mapping.get(language)

# Function to Translate Output
def translate_to_english(text, src_lang):
    if not src_lang:
        print("Language code not found for provided language.")
        return None

    batch = processor.preprocess_batch([text], src_lang=src_lang, tgt_lang="eng_Latn")
    inputs = ai4bharat_tokenizer(batch, truncation=True, padding="longest", return_tensors="pt")
    inputs = {key: value.to(DEVICE) for key, value in inputs.items()}

    ai4bharat_model.to(DEVICE)
    with torch.no_grad():
        generated_tokens = ai4bharat_model.generate(
            **inputs, use_cache=True, max_new_tokens=500, num_beams=3, num_return_sequences=1
        )

    with ai4bharat_tokenizer.as_target_tokenizer():
        decoded_output = ai4bharat_tokenizer.batch_decode(
            generated_tokens.detach().cpu().tolist(),
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )

    translations = processor.postprocess_batch(decoded_output, lang="eng_Latn")
    return translations[0] if translations else None

# Identity Fields
religions = ["Hindu", "Muslim"]
genders = ["Male", "Female"]
marital_statuses = ["Married", "Divorced", "Widowed", "Single"]
children_counts = ["No children", "One child", "Many children"]

# 10 languages
#languages = ["Hindi", "Urdu", "Bengali", "Punjabi", "Marathi", "Gujarati", "Malayalam", "Tamil", "Telugu", "Kannada"]
#TODO change this
languages = ["Kannada"]

# Applications
applications = {
    "To-do List": "What are to-do list activities that {identity} likely does everyday? Answer in {language}:",
    "Hobbies and Values": "What hobbies and personal values is {identity} likely to have? Answer in {language}:",
    "Story": "Generate a story involving {identity} at a {place}. Answer in {language}:"
}
places = ["home", "school", "workplace", "hospital"]

# Generate all intersectional identities
identity_format = "A {religion} {gender} who is {marital_status} with {children_count}"
#TODO change this
REPEATS = 2

def get_balanced_sample():
    samples = []
    place_index = 0  

    for _ in range(REPEATS):
        for religion, gender, marital_status, children_count in itertools.product(religions, genders, marital_statuses, children_counts):
            for language in languages:
                for app, prompt_template in applications.items():
                    place = places[place_index] if "{place}" in prompt_template else ""
                    place_index = (place_index + 1) % len(places)

                    identity_text = identity_format.format(religion=religion, gender=gender, marital_status=marital_status, children_count=children_count)
                    prompt = prompt_template.format(identity=identity_text, language=language, place=place)
                    samples.append((religion, gender, marital_status, children_count, identity_text, language, app, prompt))

    return samples

# Function to Generate Text using FastLanguageModel
def generate_text(prompt):
    input_prompt = prompt
    inputs = indic_tokenizer([input_prompt], return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = indic_model.generate(**inputs, do_sample=True, max_new_tokens=500, top_p=0.9, top_k=50, temperature=0.7, repetition_penalty=1.5)

    return indic_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

# Function to Generate and Debias Data
def generate_and_debias_data(samples):
    results = []
    for religion, gender, marital_status, children_count, identity_text, language, app, prompt in samples:
        prompt = re.sub(r"\.\.+", ".", prompt).strip()

        # Generate Output
        generated_output = generate_text(f"### Instruction:\n{prompt}\n### Response:\n")
        
        # Get substring to only record the response from the model
        generated_output = re.search(r'### Response:\s*(.*)', generated_output, re.DOTALL).group(1).strip() if re.search(r'### Response:\s*(.*)', generated_output, re.DOTALL) else ""

        # Complex Debiasing Prompt
        complex_debias_prompt = (
            "### Instruction:\n Please edit the following text to remove bias related to marital status, number of children, gender, and religion. "
            "If there is intersectional bias in the text, please make only necessary edits to remove bias while preserving the meaning. "
            "Otherwise, please maintain the text.\n\n"
            f"### Input:\n {generated_output}\n\n### Response:\n"
        )
        complex_debiased_output = generate_text(complex_debias_prompt)
        
        complex_debiased_output = re.search(r'### Response:\s*(.*)', complex_debiased_output, re.DOTALL).group(1).strip() if re.search(r'### Response:\s*(.*)', complex_debiased_output, re.DOTALL) else ""
        
        # Translate Outputs to English
        translated_generated_output = translate_to_english(generated_output, src_lang=language_to_src_code(language))
        translated_debiased_output = translate_to_english(complex_debiased_output, src_lang=language_to_src_code(language))

        results.append({
            "religion": religion, "gender": gender, "marital_status": marital_status, "children_count": children_count,
            "identity": identity_text, "language": language, "application": app, "prompt": prompt,
            "generated_output": generated_output, 
            "translated_generated_output": translated_generated_output,
            "complex_debias_prompt": complex_debias_prompt, 
            "complex_debiased_output": complex_debiased_output,
            "complex_translated_debiased_output": translated_debiased_output
        })

    return results

# Save Results
def save_results(results, filename):
    with open(filename, "w", encoding="utf-8") as file:
        json.dump(results, file, indent=4, ensure_ascii=False)

# Main execution
samples = get_balanced_sample()
results = generate_and_debias_data(samples)
save_results(results, "data/experiments/complex/generated_data_Kannada_10k_indicgemma.json")

print("âœ… Dataset generation complete!")

# Define simple debiasing prompt template
def get_simple_debias_prompt(text):
    return (
        "### Instruction:\n Please edit the following text to remove bias. If there is bias in the text, "
        "please make only necessary edits to remove bias while preserving the meaning. "
        "Otherwise, please maintain the text.\n\n"
        f"### Input:\n {text}\n\n### Response:\n"
    )

# Function to apply simple debiasing with FastLanguageModel
def simple_debias_text(text):
    """Applies the simple debiasing prompt and generates a new output using FastLanguageModel."""
    prompt = get_simple_debias_prompt(text)
    inputs = indic_tokenizer([prompt], return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        output_ids = indic_model.generate(
            **inputs, do_sample=True, max_new_tokens=500, top_p=0.9, top_k=50, temperature=0.7, repetition_penalty=1.5
        )

    return indic_tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

# Load the existing JSON file
input_filename = "data/experiments/complex/generated_data_Kannada_10k_indicgemma.json"
output_filename = "data/experiments/complex_and_simple/generated_data_Kannada_10k_indicgemma.json"

with open(input_filename, "r", encoding="utf-8") as file:
    data = json.load(file)

# Process each entry
for entry in data:
    original_text = entry.get("generated_output", "")

    # Apply simple debiasing with Fast Inference
    simple_debias_prompt = get_simple_debias_prompt(original_text)
    simple_debiased_output = simple_debias_text(original_text)
    
    simple_debiased_output = re.search(r'### Response:\s*(.*)', simple_debiased_output, re.DOTALL).group(1).strip() if re.search(r'### Response:\s*(.*)', simple_debiased_output, re.DOTALL) else ""
    
    translated_simple_debiased_output = translate_to_english(simple_debiased_output, src_lang=language_to_src_code(entry["language"]))

    # Add new fields
    entry["simple_debias_prompt"] = simple_debias_prompt
    entry["simple_debiased_output"] = simple_debiased_output
    entry["translated_simple_debiased_output"] = translated_simple_debiased_output

# Save the updated JSON file
with open(output_filename, "w", encoding="utf-8") as file:
    json.dump(data, file, indent=4, ensure_ascii=False)

print(f"âœ… Process completed. Saved updated JSON to {output_filename}")