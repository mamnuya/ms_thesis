# Purdah and Patriarchy: Evaluating and Mitigating South Asian Biases in Open-Ended Multilingual LLM Generations

## Introduction

Large Language Models (LLMs) are critical in AI systems, yet their deployment in culturally diverse contexts, such as South Asia, poses unique challenges. South Asian societies are deeply shaped by gender roles, religious norms, marital expectations, childbearing pressures, and practices like *purdah* and patriarchy.

Biases reflecting these cultural norms are often embedded in the training data of LLMs, posing risks of reinforcing harmful stereotypes and marginalizing vulnerable identities. Existing work on bias in LLMs lacks coverage of South Asian intersectionality across languages, culture, and real-world applications.

This project contains the code and data for model generations, evaluations, bias lexicons, and more.

## Paper
FILL THIS IN 

## Repository Structure

- ðŸ“‚ [Code Folder](./CODE): Contains scripts for model experiments, generation, bias scoring (including Bias TF-IDF), prompt formatting, and evaluation metrics.
- ðŸ“‚ [Data Folder](./data): Includes curated identity templates, translated prompts, generated outputs, the cultural bias lexicon, and calculated metrics.


## Citation

FILL THIS IN

Please cite this work if you find it useful in your own research:

```bibtex
@inproceedings{your_citation_2025,
  title={NONE},
  author={XYZ},
  year={2025},
  booktitle={NONE},
  note={NONE}
}